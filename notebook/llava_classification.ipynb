{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b75681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from model.video_emotion_dataset import EmotionDataset\n",
    "from model.llava_classifier import LLaVaEmotionClassifier as EmotionClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c0a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "GPU Memory: 6.0 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f0d859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"train_folder\": \"../data/images/images/train\",           # train/angry/, train/happy/, etc\n",
    "    \"test_folder\": \"path/to/your/test\",             # test/person_1/, test/person_2/, etc\n",
    "    \"val_folder\": \"../data/images/images/validation\",                             # Optional: validation data\n",
    "    \n",
    "    # Model settings\n",
    "    \"model_name\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    \"use_quantization\": True,                       # Set False if you have 16GB+ VRAM\n",
    "    \n",
    "    # Training parameters\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 2,                               \n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_length\": 256,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \n",
    "    # Emotion labels\n",
    "    \"emotion_labels\": ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'],\n",
    "    \n",
    "    # Output directories\n",
    "    \"output_model_dir\": \"./fine_tuned_emotion_model\",\n",
    "    \"results_dir\": \"./emotion_analysis_results\",\n",
    "    \n",
    "    # Testing settings\n",
    "    \"test_batch_size\": 6,                          \n",
    "    \"save_predictions\": True,\n",
    "}\n",
    "\n",
    "os.path.isdir(CONFIG[\"train_folder\"])\n",
    "os.path.isdir(CONFIG[\"val_folder\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb439ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training folder found: ../data/images/images/train\n",
      "   angry: 7986 images\n",
      "   disgust: 872 images\n",
      "   fear: 8206 images\n",
      "   happy: 14328 images\n",
      "   neutral: 9964 images\n",
      "   sad: 9876 images\n",
      "   surprise: 6410 images\n",
      "   Total training images: 57642\n",
      "Test folder not found: path/to/your/test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_data_structure(train_folder, test_folder, emotion_labels):\n",
    "    train_path = Path(train_folder)\n",
    "    if not train_path.exists():\n",
    "        print(f\"Training folder not found: {train_folder}\")\n",
    "        return False\n",
    "    print(f\"Training folder found: {train_folder}\")\n",
    "    train_stats = {}\n",
    "    total_train_images = 0\n",
    "\n",
    "    for emotion in emotion_labels:\n",
    "        emotion_path = train_path / emotion\n",
    "        if emotion_path.exists():\n",
    "            image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "            image_count = 0\n",
    "            for ext in image_extensions:\n",
    "                image_count += len(list(emotion_path.glob(f'*{ext}')))\n",
    "                image_count += len(list(emotion_path.glob(f\"*{ext.upper()}\")))\n",
    "\n",
    "            train_stats[emotion] = image_count\n",
    "            total_train_images += image_count\n",
    "            print(f\"   {emotion}: {image_count} images\")\n",
    "\n",
    "        else:\n",
    "            train_stats[emotion] = 0\n",
    "            print(f\"   {emotion}: Folder not found\")\n",
    "    \n",
    "    print(f\"   Total training images: {total_train_images}\")\n",
    "\n",
    "    test_path = Path(test_folder)\n",
    "    if not test_path.exists():\n",
    "        print(f\"Test folder not found: {test_folder}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nTest Data Structure:\")\n",
    "    test_stats = {}\n",
    "    total_test_images = 0\n",
    "    \n",
    "    person_folders = [d for d in test_path.iterdir() if d.is_dir()]\n",
    "    for person_folder in person_folders:\n",
    "        person_id = person_folder.name        \n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "        image_count = 0\n",
    "        for ext in image_extensions:\n",
    "            image_count += len(list(person_folder.glob(f\"*{ext}\")))\n",
    "            image_count += len(list(person_folder.glob(f\"*{ext.upper()}\")))\n",
    "        \n",
    "        test_stats[person_id] = image_count\n",
    "        total_test_images += image_count\n",
    "        print(f\"   {person_id}: {image_count} images\")\n",
    "    \n",
    "    print(f\"   Total test images: {total_test_images}\")\n",
    "    \n",
    "    # Visualization\n",
    "    create_data_visualization(train_stats, test_stats)\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_data_visualization(train_stats, test_stats):\n",
    "    \"\"\"Create visualization of data distribution\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    emotions = list(train_stats.keys())\n",
    "    counts = list(train_stats.values())\n",
    "    \n",
    "    ax1.bar(emotions, counts, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Training Data Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Emotions')\n",
    "    ax1.set_ylabel('Number of Images')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, count in enumerate(counts):\n",
    "        ax1.text(i, count + max(counts)*0.01, str(count), ha='center', va='bottom')\n",
    "    \n",
    "    persons = list(test_stats.keys())\n",
    "    test_counts = list(test_stats.values())\n",
    "    \n",
    "    ax2.bar(persons, test_counts, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Test Data Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Persons')\n",
    "    ax2.set_ylabel('Number of Images')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, count in enumerate(test_counts):\n",
    "        ax2.text(i, count + max(test_counts)*0.01, str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n Data Summary:\")\n",
    "    print(f\"   Training samples: {sum(train_stats.values())} images across {len([c for c in train_stats.values() if c > 0])} emotions\")\n",
    "    print(f\"   Test samples: {sum(test_stats.values())} images across {len(test_stats)} persons\")\n",
    "    print(f\"   Average images per emotion: {sum(train_stats.values()) / len([c for c in train_stats.values() if c > 0]):.1f}\")\n",
    "    print(f\"   Average images per person: {sum(test_stats.values()) / len(test_stats):.1f}\")\n",
    "\n",
    "\n",
    "validate_data_structure(\n",
    "    CONFIG[\"train_folder\"], \n",
    "    CONFIG[\"test_folder\"], \n",
    "    CONFIG[\"emotion_labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074eeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rtx4050_optimizations():\n",
    "    \"\"\"Apply RTX 4050 6GB optimizations before model loading\"\"\"\n",
    "    print(\"Applying RTX 4050 6GB optimizations...\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "        print(\"  Memory fraction set to 90%\")        \n",
    "        torch.backends.cuda.enable_flash_sdp(True)\n",
    "        print(\"  Flash attention enabled\")\n",
    "        \n",
    "        try:\n",
    "            torch.cuda.memory._set_allocator_settings('expandable_segments:True')\n",
    "            print(\"  Expandable segments enabled\")\n",
    "        except:\n",
    "            print(\"  Expandable segments not available (older PyTorch)\")\n",
    "        \n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\" GPU: {gpu_name}\")\n",
    "        print(f\" Total Memory: {total_memory:.1f} GB\")\n",
    "        \n",
    "        if \"4050\" in gpu_name:\n",
    "            print(\"RTX 4050 detected - optimizations applied!\")\n",
    "        elif total_memory < 7:\n",
    "            print(\" Low VRAM detected - optimizations still applied\")\n",
    "        else:\n",
    "            print(\"Optimizations applied for your GPU\")\n",
    "            \n",
    "    else:\n",
    "        print(\"CUDA not available - using CPU mode\")\n",
    "\n",
    "def monitor_memory_usage(stage=\"\"):\n",
    "    \"\"\"Monitor GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        usage_percent = (reserved / total) * 100\n",
    "        \n",
    "        print(f\" {stage} Memory Status:\")\n",
    "        print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB\") \n",
    "        print(f\"   Usage: {usage_percent:.1f}% of {total:.1f} GB\")        \n",
    "        if usage_percent > 90:\n",
    "            print(\" CRITICAL: Memory > 90% - OOM risk!\")\n",
    "            return \"critical\"\n",
    "        elif usage_percent > 80:\n",
    "            print(\"  WARNING: Memory > 80%\")\n",
    "            return \"warning\"\n",
    "        elif usage_percent > 60:\n",
    "            print(\" CAUTION: Memory > 60%\")\n",
    "            return \"caution\"\n",
    "        else:\n",
    "            print(\" GOOD: Memory usage acceptable\")\n",
    "            return \"good\"\n",
    "    return \"no_cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "541bf284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying RTX 4050 6GB optimizations...\n",
      "  Memory fraction set to 90%\n",
      "  Flash attention enabled\n",
      "  Expandable segments enabled\n",
      " GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      " Total Memory: 6.0 GB\n",
      "RTX 4050 detected - optimizations applied!\n",
      " Initial Memory Status:\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved: 0.00 GB\n",
      "   Usage: 0.0% of 6.0 GB\n",
      " GOOD: Memory usage acceptable\n",
      "Initializing Emotion Classifier...\n",
      "Initializing EmotionClassifier\n",
      "Model: llava-hf/llava-v1.6-mistral-7b-hf\n",
      "Emotions: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using enhanced 8-bit quantization for 6GB VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava_next to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m monitor_memory_usage(\u001b[33m\"\u001b[39m\u001b[33mInitial\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitializing Emotion Classifier...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m classifier = EmotionClassifier(\n\u001b[32m      7\u001b[39m     model_name=CONFIG[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m     emotion_labels=CONFIG[\u001b[33m\"\u001b[39m\u001b[33memotion_labels\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      9\u001b[39m     use_quantization=CONFIG[\u001b[33m\"\u001b[39m\u001b[33muse_quantization\u001b[39m\u001b[33m\"\u001b[39m] \n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmotionClassifier initialized!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m monitor_memory_usage(\u001b[33m\"\u001b[39m\u001b[33mAfter Model Loading\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aco\\research\\Asd-classification-image\\notebook\\..\\model\\llava_classifier.py:31\u001b[39m, in \u001b[36mLLaVaEmotionClassifier.__init__\u001b[39m\u001b[34m(self, model_name, emotion_labels, use_quantization, optimize_for_6gb)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmotions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.emotion_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m._optimize_memory_settings()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model() \n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmotionClassifier ready!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\aco\\research\\Asd-classification-image\\notebook\\..\\model\\llava_classifier.py:64\u001b[39m, in \u001b[36mLLaVaEmotionClassifier._load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m         quantization_config = BitsAndBytesConfig(\n\u001b[32m     58\u001b[39m             load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     59\u001b[39m             llm_int8_threshold=\u001b[32m6.0\u001b[39m,\n\u001b[32m     60\u001b[39m             llm_int8_has_fp16_weight=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     61\u001b[39m         )\n\u001b[32m     62\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing standard 8-bit quantization\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = LlavaForConditionalGeneration.from_pretrained(\n\u001b[32m     65\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_name,\n\u001b[32m     66\u001b[39m         quantization_config=quantization_config,\n\u001b[32m     67\u001b[39m         device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     68\u001b[39m         torch_dtype=torch.float16,\n\u001b[32m     69\u001b[39m         low_cpu_mem_usage=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     70\u001b[39m         max_memory={\u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m5.2GB\u001b[39m\u001b[33m\"\u001b[39m}, \n\u001b[32m     71\u001b[39m         attn_implementation=\u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_flash_attn_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m     )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQuantization disabled - NOT recommended for 6GB VRAM\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MufliDevs\\anaconda3\\envs\\emotion_torch\\Lib\\site-packages\\transformers\\modeling_utils.py:316\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    318\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MufliDevs\\anaconda3\\envs\\emotion_torch\\Lib\\site-packages\\transformers\\modeling_utils.py:5042\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5040\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5041\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5042\u001b[39m     device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n\u001b[32m   5044\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5045\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MufliDevs\\anaconda3\\envs\\emotion_torch\\Lib\\site-packages\\transformers\\modeling_utils.py:1501\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1498\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m         hf_quantizer.validate_environment(device_map=device_map)\n\u001b[32m   1503\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1504\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MufliDevs\\anaconda3\\envs\\emotion_torch\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py:114\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    115\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m         )\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.37.2\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    125\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have a version of `bitsandbytes` that is not compatible with 8bit inference and training\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "apply_rtx4050_optimizations()\n",
    "monitor_memory_usage(\"Initial\")\n",
    "\n",
    "print(\"Initializing Emotion Classifier...\")\n",
    "\n",
    "classifier = EmotionClassifier(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    emotion_labels=CONFIG[\"emotion_labels\"],\n",
    "    use_quantization=CONFIG[\"use_quantization\"] \n",
    ")\n",
    "\n",
    "print(\"EmotionClassifier initialized!\")\n",
    "\n",
    "monitor_memory_usage(\"After Model Loading\")\n",
    "\n",
    "print(\"\\nTesting model with sample image...\")\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    test_image = Image.new('RGB', (224, 224), color=(128, 128, 128))\n",
    "    test_result = classifier.predict_single_image(test_image)\n",
    "    \n",
    "    print(f\"Test prediction: {test_result['predicted_emotion']}\")\n",
    "    print(\"Model test successful!\")    \n",
    "    monitor_memory_usage(\"After Test\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model test failed: {e}\")\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"RTX 4050 OOM detected - try restarting notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emotion_model():\n",
    "    print(\"Starting Training Phase...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if os.path.exists(CONFIG[\"output_model_dir\"]):\n",
    "        response = input(f\"Model directory {CONFIG['output_model_dir']} exists. Overwrite? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"Training cancelled.\")\n",
    "            return\n",
    "    \n",
    "    print(\"    Training Configuration:\")\n",
    "    print(f\"   Model: {CONFIG['model_name']}\")\n",
    "    print(f\"   Training folder: {CONFIG['train_folder']}\")\n",
    "    print(f\"   Validation folder: {CONFIG['val_folder']}\")\n",
    "    print(f\"   Epochs: {CONFIG['num_epochs']}\")\n",
    "    print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "    print(f\"   Learning rate: {CONFIG['learning_rate']}\")\n",
    "    print(f\"   Quantization: {CONFIG['use_quantization']}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    print(f\"\\n Training started at: {start_time}\")\n",
    "    \n",
    "    try:\n",
    "        classifier.fine_tune(\n",
    "            train_folder=CONFIG[\"train_folder\"],\n",
    "            val_folder=CONFIG[\"val_folder\"],\n",
    "            output_dir=CONFIG[\"output_model_dir\"],\n",
    "            num_epochs=CONFIG[\"num_epochs\"],\n",
    "            batch_size=CONFIG[\"batch_size\"],\n",
    "            learning_rate=CONFIG[\"learning_rate\"]\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"\\n Training completed!\")\n",
    "        print(f\"   Duration: {duration}\")\n",
    "        print(f\"   Model saved to: {CONFIG['output_model_dir']}\")\n",
    "        \n",
    "        config_path = Path(CONFIG[\"output_model_dir\"]) / \"training_config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            training_config = CONFIG.copy()\n",
    "            training_config['training_start_time'] = start_time.isoformat()\n",
    "            training_config['training_end_time'] = end_time.isoformat()\n",
    "            training_config['training_duration'] = str(duration)\n",
    "            json.dump(training_config, f, indent=2)\n",
    "        \n",
    "        print(f\"   Training config saved to: {config_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Training failed: {e}\")\n",
    "        print(\"ðŸ’¡ Try reducing batch_size or enabling quantization if OOM error\")\n",
    "        return False\n",
    "training_success = train_emotion_model()\n",
    "\n",
    "if training_success:\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "    print(\" Ready for testing phase...\")\n",
    "else:\n",
    "    print(\"\\nTraining failed. Please check the error messages above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
